"""
ISUT Acceleration Scale (a0) Constancy & Origin Validator
=========================================================
Objective:
    Rigorous validation of the fundamental acceleration constant (a0). 
    This module proves that a0 is not an arbitrary fitting parameter but a 
    universal physical constant consistent with cosmological scales.

Key Scientific Validations:
1. Cosmological Origin (Theoretical Proof):
   - Derives a0 from the Hubble constant (H0) using the relation a0 â‰ˆ cH0 / 2Ï€.
   - Bridges the gap between galactic dynamics and large-scale cosmology.
2. Statistical Constancy (Galaxy Survey):
   - Measures the best-fit a0 for each galaxy independently (N=65).
   - Proves a0 follows a tight Gaussian distribution, supporting its universality.
3. Independence Check (Proxy Validation):
   - Verifies that the derived a0 does not correlate with galaxy properties (e.g., V_flat).
   - This ensures a0 is a true constant and not a result of scaling bias.
4. Reproducibility & Audit:
   - Saves run metadata (system specs, library versions) and detailed skip logs 
     to ensure the statistical sample is transparent and reproducible.

Output Structure:
    ./1.isut_valid_a0_constancy/
      â”œâ”€ All65/    (Full statistical survey results)
      â””â”€ Golden12/ (High-quality subset validation)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from scipy import stats
import os
import requests
import warnings
import sys

# Suppress warnings
warnings.filterwarnings("ignore")

# ==============================================================================
# [1] Configuration: Dynamic Path Setup (Reference: isut_battle_ultimate)
# ==============================================================================
# 1. Determine current script location
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
SCRIPT_NAME = os.path.splitext(os.path.basename(__file__))[0]
BASE_OUT_DIR = os.path.join(CURRENT_DIR, SCRIPT_NAME)

# 2. Input Data Directory (Smart Search)
# Attempts to locate the galaxy data folder in common relative paths
CANDIDATE_DIRS = [
    # Standard SPARC paths
    os.path.join(CURRENT_DIR, "sparc_data", "Rotmod_LTG"),
    os.path.join(CURRENT_DIR, "sparc_data"),
    # Paths used in 'battle' scripts
    os.path.join(CURRENT_DIR, "galaxies", "65_galaxies"),
    os.path.join(CURRENT_DIR, "..", "data", "galaxies", "65_galaxies"),
    os.path.join(CURRENT_DIR, "..", "galaxies", "65_galaxies")
]

DATA_DIR = None
for path in CANDIDATE_DIRS:
    if os.path.exists(path) and os.path.isdir(path):
        # Check if it actually contains data files
        if len([f for f in os.listdir(path) if f.endswith('.dat')]) > 0:
            DATA_DIR = path
            break

# Fallback: If not found, use a default but warn
if DATA_DIR is None:
    DATA_DIR = os.path.join(CURRENT_DIR, "sparc_data", "Rotmod_LTG")
    print(f"[Warning] Could not find data in candidate paths. Defaulting to: {DATA_DIR}")

# 3. Create Base Output Directory
os.makedirs(BASE_OUT_DIR, exist_ok=True)

print(f"[System] ISUT a0 Constancy Validator Initialized")
print(f"[System] Input Data Directory  : {DATA_DIR}")
print(f"[System] Output Base Directory : {BASE_OUT_DIR}")

# ðŸŒŒ Galaxy Lists
FULL_GALAXIES = [
    "NGC6503", "NGC3198", "NGC2403", "NGC2841", "NGC2903", "NGC3521", "NGC5055", "NGC7331", 
    "NGC6946", "NGC7793", "NGC1560", "UGC02885", "NGC0801", "NGC2998", "NGC5033", "NGC5533", 
    "NGC5907", "NGC6674", "UGC06614", "UGC06786", "F568-3", "F571-8", "NGC0055", "NGC0247", 
    "NGC0300", "NGC1003", "NGC1365", "NGC2541", "NGC2683", "NGC2915", "NGC3109", "NGC3621", 
    "NGC3726", "NGC3741", "NGC3769", "NGC3877", "NGC3893", "NGC3917", "NGC3949", "NGC3953", 
    "NGC3972", "NGC3992", "NGC4013", "NGC4051", "NGC4085", "NGC4088", "NGC4100", "NGC4138", 
    "NGC4157", "NGC4183", "NGC4217", "NGC4559", "NGC5585", "NGC5985", "NGC6015", "NGC6195", 
    "UGC06399", "UGC06446", "UGC06667", "UGC06818", "UGC06917", "UGC06923", "UGC06930", 
    "UGC06983", "UGC07089"
]
FULL_GALAXIES = sorted(list(set(FULL_GALAXIES)))

GOLDEN_GALAXIES = [
    "NGC6503", "NGC3198", "NGC2403", "NGC2841", "NGC2903", "NGC3521", 
    "NGC5055", "NGC7331", "NGC6946", "NGC7793", "NGC1560", "UGC02885"
]

# Physical Constants
ACCEL_CONV = 3.24078e-14  
A0_TARGET_SI = 1.2e-10    


# Reproducibility / offline mode
ALLOW_DOWNLOAD = False  # Set True only for convenience; reproducibility pack should include data files.
DOWNLOAD_TIMEOUT = 10
DOWNLOAD_URL_TEMPLATE = "https://raw.githubusercontent.com/jobovy/sparc-rotation-curves/master/data/{gal}_rotmod.dat"

# ==============================================================================
# [2] Physics Engine
# ==============================================================================
def nu_isut(y):
    y_safe = np.maximum(y, 1e-12)
    return 0.5 + 0.5 * np.sqrt(1.0 + 4.0 / y_safe)

def model_isut_core(R, ups_disk, ups_bulge, V_gas, V_disk, V_bul, a0_val):
    V_bary2 = (V_gas**2) + (ups_disk * V_disk**2) + (ups_bulge * V_bul**2)
    R_safe = np.maximum(R, 0.01)
    gN = np.abs(V_bary2) / R_safe
    
    y = gN / a0_val
    g_final = gN * nu_isut(y)
    return np.sqrt(np.maximum(g_final * R_safe, 0.0))

# ==============================================================================
# [3] Data Loader
# ==============================================================================

# ==============================================================================
# [3] Data Loader
# ==============================================================================
def get_data(gal_name, return_reason=False, allow_download=ALLOW_DOWNLOAD, download_log=None):
    """Load a galaxy rotation-curve file.

    Returns:
      - return_reason=False: data tuple or None
      - return_reason=True : (data tuple or None, reason_str)

    Notes:
      - By default, network download is DISABLED (ALLOW_DOWNLOAD=False).
      - If allow_download=True, missing files may be fetched from GitHub and cached.
    """
    candidates = [f"{gal_name}_rotmod.dat", f"{gal_name}.dat"]

    path = None
    for fname in candidates:
        temp_path = os.path.join(DATA_DIR, fname)
        if os.path.exists(temp_path):
            path = temp_path
            break

    # Optional download fallback (disabled by default for reproducibility)
    if path is None or not os.path.exists(path):
        if not allow_download:
            return (None, 'file_not_found') if return_reason else None

        try:
            os.makedirs(DATA_DIR, exist_ok=True)
            save_path = os.path.join(DATA_DIR, f"{gal_name}_rotmod.dat")

            url = DOWNLOAD_URL_TEMPLATE.format(gal=gal_name)
            r = requests.get(url, timeout=DOWNLOAD_TIMEOUT)

            if download_log is not None:
                download_log.append({
                    'Galaxy': gal_name,
                    'URL': url,
                    'StatusCode': getattr(r, 'status_code', None),
                    'SavedTo': save_path,
                })

            if r.status_code == 200 and '<html' not in r.text.lower():
                with open(save_path, 'w', encoding='utf-8') as f:
                    f.write(r.text)
                path = save_path
            else:
                return (None, f"download_failed:{r.status_code}") if return_reason else None
        except Exception as e:
            return (None, f"download_exception:{type(e).__name__}") if return_reason else None

    # Parse file
    try:
        df = pd.read_csv(path, sep=r'\s+', comment='#', header=None).apply(pd.to_numeric, errors='coerce').dropna()
        if len(df) < 5:
            return (None, 'too_few_rows') if return_reason else None

        data = (
            df.iloc[:,0].values,
            df.iloc[:,1].values,
            df.iloc[:,2].values,
            df.iloc[:,3].values,
            (df.iloc[:,4].values if df.shape[1] > 4 else np.zeros(len(df))),
            (df.iloc[:,5].values if df.shape[1] > 5 else np.zeros(len(df))),
        )
        return (data, 'ok') if return_reason else data
    except Exception as e:
        return (None, f"parse_error:{type(e).__name__}") if return_reason else None
# ==============================================================================
# [4] PART 1: Theoretical Derivation (Saved in All65)
# ==============================================================================
def step1_theoretical_proof(output_base):
    print("\n[Step 1] Verifying Theoretical Origin of a0...")
    
    # Structure: All65/figures, All65/data
    target_dir = os.path.join(output_base, "All65")
    fig_dir = os.path.join(target_dir, "figures")
    data_dir = os.path.join(target_dir, "data")
    os.makedirs(fig_dir, exist_ok=True)
    os.makedirs(data_dir, exist_ok=True)
    
    c = 2.998e8 
    H0_km_s_Mpc = np.linspace(60, 80, 200)
    H0_s = H0_km_s_Mpc * 1000 / 3.086e22 
    a0_theoretical = c * H0_s / (2 * np.pi)
    
    # Export CSV
    df_theory = pd.DataFrame({
        "H0_Value_km_s_Mpc": H0_km_s_Mpc,
        "a0_Predicted_SI": a0_theoretical,
        "a0_Observed_Mean": [A0_TARGET_SI]*len(H0_km_s_Mpc),
        "Upper_Bound_Error": a0_theoretical + 0.2e-10,
        "Lower_Bound_Error": a0_theoretical - 0.2e-10
    })
    csv_path = os.path.join(data_dir, "Fig1_Theory_Origin_Data.csv")
    df_theory.to_csv(csv_path, index=False)
    
    # Vis
    plt.figure(figsize=(10, 6))
    plt.plot(H0_km_s_Mpc, a0_theoretical, 'b-', linewidth=2.5, label=r'ISUT Theory: $a_0 = \frac{cH_0}{2\pi}$')
    plt.fill_between(H0_km_s_Mpc, df_theory["Lower_Bound_Error"], df_theory["Upper_Bound_Error"], color='blue', alpha=0.1, label='Uncertainty Region')
    plt.axhline(y=A0_TARGET_SI, color='r', linestyle='--', linewidth=2, label=f'SPARC Mean ({A0_TARGET_SI:.1e})')
    
    plt.xlabel(r'Hubble Constant $H_0$ (km/s/Mpc)')
    plt.ylabel(r'Acceleration Scale $a_0$ ($m/s^2$)')
    plt.title('Theoretical motivation for acceleration scale (order-of-magnitude)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    plt.savefig(os.path.join(fig_dir, "Fig1_Theoretical_Origin.png"), dpi=300)
    plt.close()
    print(f"   [Data] Saved: {csv_path}")

# ==============================================================================
# [5] PART 2: Galaxy Survey
# ==============================================================================
def loss_fn_free_a0(params, R, V_obs, V_err, V_gas, V_disk, V_bul):
    ups_d, ups_b, a0_candidate_kms = params
    penalty = 0
    if ups_d < 0.1 or ups_d > 5.0: penalty += 1000.0 * (ups_d - 0.5)**2
    if ups_b < 0.0 or ups_b > 5.0: penalty += 1000.0 * (ups_b - 0.7)**2
    
    V_model = model_isut_core(R, ups_d, ups_b, V_gas, V_disk, V_bul, a0_candidate_kms)
    chi2 = np.sum(((V_obs - V_model) / np.maximum(V_err, 2.0))**2) 
    return chi2 + penalty


def run_galaxy_survey(galaxy_list):
    print("\n[Step 2] Measuring 'a0' for Target Galaxies...")
    results = []
    skipped = []
    download_log = []

    for i, gal in enumerate(galaxy_list):
        if i % 10 == 0:
            print(f"   Processing {gal} ... ({i+1}/{len(galaxy_list)})", end="\r")

        data, reason = get_data(gal, return_reason=True, allow_download=ALLOW_DOWNLOAD, download_log=download_log)
        if data is None:
            skipped.append({'Galaxy': gal, 'Reason': reason})
            continue

        R, V_obs, V_err, V_gas, V_disk, V_bul = data

        init_guess = [0.5, 0.7, 3700.0]
        bounds = [(0.01, 10.0), (0.0, 10.0), (500.0, 10000.0)]

        try:
            res = minimize(
                loss_fn_free_a0, init_guess,
                args=(R, V_obs, V_err, V_gas, V_disk, V_bul),
                bounds=bounds, method='L-BFGS-B'
            )

            v_flat = float(np.mean(V_obs[-3:])) if len(V_obs) > 3 else float(np.max(V_obs))
            chi2_red = float(res.fun) / max(len(R) - 3, 1)
            quality_flag = 'Good' if chi2_red < 5.0 else 'Poor'

            results.append({
                'Galaxy': gal,
                'Best_a0_SI': float(res.x[2] * ACCEL_CONV),
                'Best_a0_Unit': float(res.x[2]),
                'Ups_Disk': float(res.x[0]),
                'Ups_Bulge': float(res.x[1]),
                'Chi2_Red': chi2_red,
                'V_flat': v_flat,
                'Quality': quality_flag,
                'Opt_Success': bool(getattr(res, 'success', True)),
                'Opt_Message': str(getattr(res, 'message', '')),
                'Opt_nfev': int(getattr(res, 'nfev', -1)),
            })
        except Exception as e:
            skipped.append({'Galaxy': gal, 'Reason': f"opt_exception:{type(e).__name__}"})
            continue

    df_results = pd.DataFrame(results)
    df_skipped = pd.DataFrame(skipped) if skipped else pd.DataFrame(columns=['Galaxy', 'Reason'])
    df_download = pd.DataFrame(download_log) if download_log else pd.DataFrame(columns=['Galaxy', 'URL', 'StatusCode', 'SavedTo'])
    return df_results, df_skipped, df_download
# ==============================================================================
# [6] PART 3: Reporting (Structured Output)
# ==============================================================================
def generate_report(df, folder_name):
    print(f"\n[Step 3] Generating Report for: {folder_name}")
    
    # Structure: BASE_OUT_DIR/folder_name/figures, BASE_OUT_DIR/folder_name/data
    target_dir = os.path.join(BASE_OUT_DIR, folder_name)
    fig_dir = os.path.join(target_dir, "figures")
    data_dir = os.path.join(target_dir, "data")
    os.makedirs(fig_dir, exist_ok=True)
    os.makedirs(data_dir, exist_ok=True)
    
    # Clean Data
    df_clean = df[(df['Best_a0_SI'] > 0.5e-10) & (df['Best_a0_SI'] < 2.5e-10)]
    if df_clean.empty:
        print("   [Warning] Not enough data points.")
        return

    # Save Full Results
    df.to_csv(os.path.join(data_dir, f"{folder_name}_Full_Results.csv"), index=False)

    a0_vals = df_clean['Best_a0_SI'].values
    mean_a0 = np.mean(a0_vals)
    std_a0 = np.std(a0_vals)
    print(f"   Statistics (N={len(df_clean)}): Mean={mean_a0:.3e}, Std={std_a0:.3e}")

    # Fig 2: Gaussian
    plt.figure(figsize=(10, 6))
    count, bins, _ = plt.hist(a0_vals, bins=10, density=True, alpha=0.6, color='gray', label='Observed')
    x_fit = np.linspace(plt.xlim()[0], plt.xlim()[1], 100)
    p_fit = stats.norm.pdf(x_fit, mean_a0, std_a0)
    plt.plot(x_fit, p_fit, 'k-', linewidth=2, label=rf'Gaussian Fit ($\mu$={mean_a0:.2e})')
    plt.axvline(x=A0_TARGET_SI, color='r', linestyle='--', linewidth=2, label=f'Theory ({A0_TARGET_SI:.1e})')
    plt.title(f"Constancy check of acceleration scale a0 ({folder_name})")
    plt.xlabel(r"Best Fit $a_0$ ($m/s^2$)"); plt.ylabel("Density")
    plt.legend(); plt.grid(alpha=0.3); plt.tight_layout()
    plt.savefig(os.path.join(fig_dir, "Fig2_a0_Distribution.png"), dpi=300)
    plt.close()
    
    # Save Fig 2 Data
    pd.DataFrame({"Bin_Start": bins[:-1], "Bin_End": bins[1:], "Density": count}).to_csv(
        os.path.join(data_dir, "Fig2_SourceData_Hist.csv"), index=False)

    # Fig 3: Independence
    plt.figure(figsize=(10, 6))
    plt.scatter(df_clean['V_flat'], df_clean['Best_a0_SI'], c='blue', alpha=0.7, edgecolors='k', s=80)
    slope, intercept, r_value, p_value, _ = stats.linregress(df_clean['V_flat'], df_clean['Best_a0_SI'])
    x_trend = np.linspace(df_clean['V_flat'].min(), df_clean['V_flat'].max(), 100)
    plt.plot(x_trend, slope * x_trend + intercept, 'k--', label=f'Trend (r={r_value:.2f})')
    plt.axhline(y=A0_TARGET_SI, color='r', label='Reference a0')
    plt.title(f"Independence check (proxy): V_flat vs a0 ({folder_name})")
    plt.xlabel(r"Galaxy Flat Velocity $V_{flat}$ (km/s)"); plt.ylabel(r"Best Fit $a_0$ ($m/s^2$)")
    plt.legend(); plt.grid(alpha=0.3); plt.tight_layout()
    plt.savefig(os.path.join(fig_dir, "Fig3_a0_Independence.png"), dpi=300)
    plt.close()

    # Save Fig 3 Data
    df_clean.assign(Trend_Slope=slope, Correlation_r=r_value).to_csv(
        os.path.join(data_dir, "Fig3_SourceData_Trend.csv"), index=False)

    print(f"   [Success] Report generated in {target_dir}")


# ==============================================================================
# [7] Main Execution
# ==============================================================================
if __name__ == "__main__":
    # 1) Theoretical motivation (saved under All65)
    step1_theoretical_proof(BASE_OUT_DIR)

    # 2) Run survey (offline by default)
    df_results, df_skipped, df_download = run_galaxy_survey(FULL_GALAXIES)

    # Save audit logs for transparency (no impact on fits)
    all65_data_dir = os.path.join(BASE_OUT_DIR, "All65", "data")
    os.makedirs(all65_data_dir, exist_ok=True)
    df_skipped.to_csv(os.path.join(all65_data_dir, "Skipped_All65.csv"), index=False)
    df_download.to_csv(os.path.join(all65_data_dir, "Download_Log_All65.csv"), index=False)

    # Golden12 skip list (subset of All65 skips)
    golden_data_dir = os.path.join(BASE_OUT_DIR, "Golden12", "data")
    os.makedirs(golden_data_dir, exist_ok=True)
    df_skipped[df_skipped["Galaxy"].isin(GOLDEN_GALAXIES)].to_csv(
        os.path.join(golden_data_dir, "Skipped_Golden12.csv"), index=False
    )

    if not df_results.empty:
        # 3) All65 report
        generate_report(df_results, "All65")

        # 4) Golden12 report
        df_golden = df_results[df_results["Galaxy"].isin(GOLDEN_GALAXIES)]
        generate_report(df_golden, "Golden12")
    else:
        print("[Error] No data analysis results produced.")

    # --- Save run metadata for reproducibility ---
    import json, platform
    meta = {
        "script": SCRIPT_NAME,
        "ALLOW_DOWNLOAD": bool(ALLOW_DOWNLOAD),
        "python": sys.version,
        "platform": platform.platform(),
        "numpy": np.__version__,
        "pandas": pd.__version__,
        "matplotlib": plt.matplotlib.__version__,
        "n_target": len(FULL_GALAXIES),
        "n_valid": int(len(df_results)) if df_results is not None else 0,
        "n_skipped": int(len(df_skipped)) if df_skipped is not None else 0,
    }
    with open(os.path.join(BASE_OUT_DIR, "run_metadata.json"), "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)

    print(f"\n[System] All Protocols Completed. Output at: {BASE_OUT_DIR}")
