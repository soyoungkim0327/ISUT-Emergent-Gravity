# -*- coding: utf-8 -*-
"""
Reproduce a0 Constancy (All65) from Existing Outputs
====================================================

Purpose
-------
This script DOES NOT rerun galaxy fits.
It reproduces the key "a0 constancy" artifacts strictly from existing result CSVs,
primarily those generated by:
  isut_300_valid_a0_constancy.py  (validator)

Reviewer-facing goals:
- Regenerate Fig2: a0 distribution (histogram + Gaussian overlay)
- Regenerate Fig3: independence check (V_flat vs a0) with regression + r
- Save ALL plots AND their source CSV tables (audit-ready)
- Provide a correlation table across available numeric proxies (optional)

Expected validator output structure (preferred input source)
----------------------------------------------------------
./isut_300_valid_a0_constancy/
  ├─ All65/
  │   ├─ figures/
  │   └─ data/
  │       └─ All65_Full_Results.csv
  └─ Golden12/ ...

Outputs (this script)
---------------------
./<SCRIPT_NAME>/
  ├─ All65/
  │   ├─ figures/
  │   │   ├─ Fig2_a0_Distribution.png
  │   │   ├─ Fig3_a0_Independence.png
  │   │   └─ a0_vs_<var>.png (optional)
  │   └─ data/
  │       ├─ All65_Full_Results_USED.csv (copy of the exact input used)
  │       ├─ Fig2_SourceData_Hist.csv
  │       ├─ Fig3_SourceData_Trend.csv
  │       └─ a0_correlation_table.csv
  └─ run_metadata.json

Notes
-----
- Uses matplotlib Agg backend for headless runs.
- Uses only numpy/pandas (no SciPy dependency required).
"""

from __future__ import annotations

import os
import sys
import json
import time
import argparse
import platform
import numpy as np
import pandas as pd

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt


# ==============================================================================
# [0] Path conventions (matches your project style)
# ==============================================================================
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
SCRIPT_NAME = os.path.splitext(os.path.basename(__file__))[0]
BASE_OUT_DIR = os.path.join(CURRENT_DIR, SCRIPT_NAME)
os.makedirs(BASE_OUT_DIR, exist_ok=True)

print(f"[System] {SCRIPT_NAME} initialized")
print(f"[Info] Current Dir : {CURRENT_DIR}")
print(f"[Info] Output Base : {BASE_OUT_DIR}")


# ==============================================================================
# [1] Preferred validator output discovery
# ==============================================================================
VALIDATOR_DIR_CANDIDATES = [
    os.path.join(CURRENT_DIR, "isut_300_valid_a0_constancy"),
    os.path.join(CURRENT_DIR, "..", "isut_300_valid_a0_constancy"),
    os.path.join(CURRENT_DIR, "03.Advanced_Validation", "isut_300_valid_a0_constancy"),
    os.path.join(CURRENT_DIR, "..", "03.Advanced_Validation", "isut_300_valid_a0_constancy"),
]

PREFERRED_RELATIVE = os.path.join("All65", "data", "All65_Full_Results.csv")

# Fallback search candidates (compatible with your old reproduce script)
CANDIDATE_FILENAMES = [
    "All65_Full_Results.csv",
    "All65_Holdout_Results.csv",
    "Holdout_Summary_Report.csv",
    "Batch_Summary_All65.csv",
    "Batch_Summary_All65_Full.csv",
    "All65_Summary.csv",
]

DEFAULT_SEARCH_ROOT = os.environ.get("ISUT_SEARCH_ROOT", CURRENT_DIR)


def deep_scan_for_csv(search_root: str, candidate_names: list[str]) -> list[str]:
    found = []
    for root, _, files in os.walk(search_root):
        for fn in files:
            if fn in candidate_names:
                found.append(os.path.join(root, fn))
    return found


def find_preferred_all65_csv() -> str | None:
    for vdir in VALIDATOR_DIR_CANDIDATES:
        p = os.path.join(vdir, PREFERRED_RELATIVE)
        if os.path.exists(p):
            return p
    return None


# ==============================================================================
# [2] Column detection utilities
# ==============================================================================
A0_COL_PRIORITY = [
    "Best_a0_SI",   # validator output
    "Best_a0",      # other scripts
    "a0_SI",
    "a0_si",
    "a0",
]

VFLAT_COL_PRIORITY = [
    "V_flat", "v_flat", "Vflat", "VFLAT"
]

GAL_COL_PRIORITY = [
    "Galaxy", "galaxy", "Name", "name", "ID", "id"
]


def pick_col(df: pd.DataFrame, priority: list[str]) -> str | None:
    lower_map = {c.lower(): c for c in df.columns}
    for key in priority:
        if key.lower() in lower_map:
            return lower_map[key.lower()]
    return None


def to_num(s: pd.Series) -> pd.Series:
    return pd.to_numeric(s, errors="coerce")


# ==============================================================================
# [3] Basic stats helpers (no SciPy)
# ==============================================================================
def normal_pdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:
    sigma = max(float(sigma), 1e-300)
    z = (x - mu) / sigma
    return (1.0 / (sigma * np.sqrt(2.0 * np.pi))) * np.exp(-0.5 * z * z)


def pearson_r(x: np.ndarray, y: np.ndarray) -> float:
    if len(x) < 3:
        return float("nan")
    x = x - np.mean(x)
    y = y - np.mean(y)
    denom = np.sqrt(np.sum(x * x)) * np.sqrt(np.sum(y * y))
    if denom <= 0:
        return float("nan")
    return float(np.sum(x * y) / denom)


def spearman_r(x: np.ndarray, y: np.ndarray) -> float:
    xr = pd.Series(x).rank(method="average").to_numpy()
    yr = pd.Series(y).rank(method="average").to_numpy()
    return pearson_r(xr, yr)


# ==============================================================================
# [4] Main reproduction logic
# ==============================================================================
def reproduce_all65(
    input_csv: str,
    out_base: str,
    a0_target_si: float = 1.2e-10,
    clean_min: float = 0.5e-10,
    clean_max: float = 2.5e-10,
    bins: int = 10,
    top_k: int = 6,
) -> None:
    """
    Reproduce:
      - Fig2_a0_Distribution.png + Fig2_SourceData_Hist.csv
      - Fig3_a0_Independence.png + Fig3_SourceData_Trend.csv
      - a0_correlation_table.csv and optional a0_vs_<var>.png

    The cleaning window matches the validator's report logic:
      0.5e-10 < a0 < 2.5e-10  (in SI) :contentReference[oaicite:5]{index=5}
    """

    # Output structure
    target_dir = os.path.join(out_base, "All65")
    fig_dir = os.path.join(target_dir, "figures")
    data_dir = os.path.join(target_dir, "data")
    os.makedirs(fig_dir, exist_ok=True)
    os.makedirs(data_dir, exist_ok=True)

    # Load input
    df = pd.read_csv(input_csv)
    print(f"[Load] {input_csv} (rows={len(df)}, cols={len(df.columns)})")

    # Identify columns
    a0_col = pick_col(df, A0_COL_PRIORITY)
    if a0_col is None:
        raise ValueError(f"Could not find a0 column. Tried: {A0_COL_PRIORITY}")

    vflat_col = pick_col(df, VFLAT_COL_PRIORITY)
    gal_col = pick_col(df, GAL_COL_PRIORITY)

    # Coerce numeric
    df[a0_col] = to_num(df[a0_col])
    if vflat_col and vflat_col in df.columns:
        df[vflat_col] = to_num(df[vflat_col])

    # Save exact input copy used (audit)
    used_copy = os.path.join(data_dir, "All65_Full_Results_USED.csv")
    df.to_csv(used_copy, index=False)
    print(f"[Saved] {used_copy}")

    # Clean according to validator window (SI only makes sense if a0 is SI)
    df_clean = df.dropna(subset=[a0_col]).copy()

    # Apply cleaning window only if this appears to be SI-scale (heuristic):
    # If median a0 is ~1e-10 order, treat as SI and apply window.
    med_a0 = float(np.nanmedian(df_clean[a0_col].values))
    is_si_like = (1e-13 < med_a0 < 1e-7)

    if is_si_like:
        df_clean = df_clean[(df_clean[a0_col] > clean_min) & (df_clean[a0_col] < clean_max)].copy()

    if len(df_clean) < 8:
        raise ValueError(
            f"Not enough valid rows after cleaning (N={len(df_clean)}). "
            f"a0_col={a0_col}, median={med_a0:.3e}, is_si_like={is_si_like}"
        )

    a0_vals = df_clean[a0_col].values.astype(float)
    mu = float(np.mean(a0_vals))
    sigma = float(np.std(a0_vals, ddof=0))

    print(f"[Stats] N={len(df_clean)} mean={mu:.3e} std={sigma:.3e}  (a0_col={a0_col})")

    # ----------------------------------------------------------------------
    # Fig 2: Distribution + Gaussian overlay (match validator semantics)
    # ----------------------------------------------------------------------
    fig2_png = os.path.join(fig_dir, "Fig2_a0_Distribution.png")
    fig2_hist_csv = os.path.join(data_dir, "Fig2_SourceData_Hist.csv")

    plt.figure(figsize=(10, 6))
    # density=True to match validator output style :contentReference[oaicite:6]{index=6}
    count, bin_edges, _ = plt.hist(a0_vals, bins=bins, density=True, alpha=0.6, color="gray", label="Observed")

    x_fit = np.linspace(float(np.min(a0_vals)), float(np.max(a0_vals)), 200)
    p_fit = normal_pdf(x_fit, mu, sigma)
    plt.plot(x_fit, p_fit, "k-", linewidth=2, label=rf"Gaussian fit ($\mu$={mu:.2e})")

    if is_si_like:
        plt.axvline(x=a0_target_si, color="r", linestyle="--", linewidth=2, label=f"Reference ({a0_target_si:.1e})")

    plt.title("Constancy check of acceleration scale a0 (All65, reproduced)")
    plt.xlabel(r"Best fit $a_0$ ($m/s^2$)" if is_si_like else f"{a0_col} (units as in CSV)")
    plt.ylabel("Density")
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(fig2_png, dpi=300)
    plt.close()
    print(f"[Saved] {fig2_png}")

    # Save Fig2 source data
    hist_df = pd.DataFrame({
        "Bin_Start": bin_edges[:-1],
        "Bin_End": bin_edges[1:],
        "Density": count
    })
    hist_df.to_csv(fig2_hist_csv, index=False)
    print(f"[Saved] {fig2_hist_csv}")

    # ----------------------------------------------------------------------
    # Fig 3: Independence check (V_flat vs a0) + regression + r
    # ----------------------------------------------------------------------
    if vflat_col is not None and vflat_col in df_clean.columns:
        fig3_png = os.path.join(fig_dir, "Fig3_a0_Independence.png")
        fig3_trend_csv = os.path.join(data_dir, "Fig3_SourceData_Trend.csv")

        x = df_clean[vflat_col].values.astype(float)
        y = df_clean[a0_col].values.astype(float)

        m = np.isfinite(x) & np.isfinite(y)
        x = x[m]
        y = y[m]

        # Linear regression (numpy-only)
        slope, intercept = np.polyfit(x, y, 1)
        r_val = pearson_r(x, y)

        x_trend = np.linspace(float(np.min(x)), float(np.max(x)), 200)
        y_trend = slope * x_trend + intercept

        plt.figure(figsize=(10, 6))
        plt.scatter(x, y, alpha=0.7, edgecolors="k", s=80)
        plt.plot(x_trend, y_trend, "k--", label=f"Trend (r={r_val:.2f})")

        if is_si_like:
            plt.axhline(y=a0_target_si, color="r", label="Reference a0")

        plt.title("Independence check (proxy): V_flat vs a0 (All65, reproduced)")
        plt.xlabel(r"Galaxy flat velocity $V_{flat}$ (km/s)" if vflat_col.lower() == "v_flat" else vflat_col)
        plt.ylabel(r"Best fit $a_0$ ($m/s^2$)" if is_si_like else a0_col)
        plt.legend()
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(fig3_png, dpi=300)
        plt.close()
        print(f"[Saved] {fig3_png}")

        # Save Fig3 source data
        out_df = df_clean.copy()
        out_df["Trend_Slope"] = slope
        out_df["Trend_Intercept"] = intercept
        out_df["Correlation_r"] = r_val
        out_df.to_csv(fig3_trend_csv, index=False)
        print(f"[Saved] {fig3_trend_csv}")
    else:
        print("[Warn] V_flat column not found; skipping Fig3 (independence check).")

    # ----------------------------------------------------------------------
    # Correlation table across numeric proxies (reviewer-facing)
    # ----------------------------------------------------------------------
    corr_csv = os.path.join(data_dir, "a0_correlation_table.csv")

    # Numeric columns excluding identifiers and a0 itself
    exclude = {a0_col}
    if gal_col:
        exclude.add(gal_col)

    numeric_cols = []
    for c in df_clean.columns:
        if c in exclude:
            continue
        if pd.api.types.is_numeric_dtype(df_clean[c]):
            numeric_cols.append(c)

    rows = []
    for c in numeric_cols:
        xx = to_num(df_clean[c]).values.astype(float)
        yy = df_clean[a0_col].values.astype(float)
        m = np.isfinite(xx) & np.isfinite(yy)
        if int(np.sum(m)) < 8:
            continue
        rp = pearson_r(xx[m], yy[m])
        rs = spearman_r(xx[m], yy[m])
        rows.append({
            "variable": c,
            "N": int(np.sum(m)),
            "pearson_r": rp,
            "spearman_r": rs,
            "abs_pearson_r": abs(rp) if np.isfinite(rp) else np.nan,
        })

    corr_df = pd.DataFrame(rows).sort_values("abs_pearson_r", ascending=False)
    corr_df.to_csv(corr_csv, index=False)
    print(f"[Saved] {corr_csv}")

    # Optional: plot a0 vs top-k variables (small, reviewer-friendly)
    top_vars = corr_df["variable"].head(max(0, int(top_k))).tolist()

    for var in top_vars:
        xx = to_num(df_clean[var]).values.astype(float)
        yy = df_clean[a0_col].values.astype(float)
        m = np.isfinite(xx) & np.isfinite(yy)
        if int(np.sum(m)) < 8:
            continue

        rp = pearson_r(xx[m], yy[m])
        rs = spearman_r(xx[m], yy[m])

        out_png = os.path.join(fig_dir, f"a0_vs_{var}.png")
        plt.figure(figsize=(8, 6))
        plt.scatter(xx[m], yy[m], s=18)
        plt.xlabel(var)
        plt.ylabel(r"Best fit $a_0$ ($m/s^2$)" if is_si_like else a0_col)
        plt.title(f"a0 vs {var} (Pearson r={rp:.3f}, Spearman r={rs:.3f})")
        plt.grid(True, linestyle=":")
        plt.tight_layout()
        plt.savefig(out_png, dpi=300)
        plt.close()
        print(f"[Saved] {out_png}")

    # Run metadata
    meta = {
        "script": SCRIPT_NAME,
        "input_csv": input_csv,
        "a0_col": a0_col,
        "vflat_col": vflat_col,
        "galaxy_col": gal_col,
        "N_raw": int(len(df)),
        "N_used": int(len(df_clean)),
        "a0_median_raw": med_a0,
        "is_si_like": bool(is_si_like),
        "clean_window_applied": bool(is_si_like),
        "clean_min": float(clean_min),
        "clean_max": float(clean_max),
        "python": sys.version,
        "platform": platform.platform(),
        "numpy": np.__version__,
        "pandas": pd.__version__,
        "matplotlib": matplotlib.__version__,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
    }
    with open(os.path.join(out_base, "run_metadata.json"), "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)
    print(f"[Saved] {os.path.join(out_base, 'run_metadata.json')}")


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Reproduce a0 constancy artifacts from existing All65 CSV outputs (no refit)."
    )
    parser.add_argument("--csv", type=str, default=None, help="Explicit path to All65_Full_Results.csv.")
    parser.add_argument("--search-root", type=str, default=DEFAULT_SEARCH_ROOT, help="Deep-scan root when --csv not given.")
    parser.add_argument("--bins", type=int, default=10, help="Histogram bins for Fig2.")
    parser.add_argument("--top-k", type=int, default=6, help="Top-k variables to plot in a0_vs_<var>.png.")
    args = parser.parse_args()

    # Prefer validator structure
    csv_path = args.csv
    if csv_path is None:
        csv_path = find_preferred_all65_csv()

    # Fallback deep scan
    if csv_path is None:
        found = deep_scan_for_csv(args.search_root, CANDIDATE_FILENAMES)
        if not found:
            raise FileNotFoundError(
                "No candidate CSV found. Provide --csv explicitly or set ISUT_SEARCH_ROOT."
            )
        found_sorted = sorted(found, key=lambda p: os.path.getmtime(p), reverse=True)
        csv_path = found_sorted[0]

    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV not found: {csv_path}")

    reproduce_all65(
        input_csv=csv_path,
        out_base=BASE_OUT_DIR,
        bins=int(args.bins),
        top_k=int(args.top_k),
    )

    print(f"[Done] Outputs written under: {BASE_OUT_DIR}")


if __name__ == "__main__":
    main()
